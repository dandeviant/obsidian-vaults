![[robots.png]]

[Google Search Central - Introduction to robots.txt](https://developers.google.com/search/docs/advanced/robots/intro)

A **robots.txt** file tells search engine crawlers which URLs the crawler can access on your site. This is used mainly to avoid overloading your site with requests

Sample of robots.txt
```text
User-agent: *
Disallow: /*__*
```

Use robots.txt to find any disallow path, the path shown may be important for us to find the CTF flag



